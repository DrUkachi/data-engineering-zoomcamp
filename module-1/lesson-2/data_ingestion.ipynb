{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.10.tar.gz (385 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.10-cp310-cp310-linux_x86_64.whl size=167125 sha256=e1285ee942adbe575d75f86a28c8e25919ee497269031a1f3eb539775e3de4c4\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/51/41/e0/2912ad51b01f454d26dfb26e5cc5923874656749b9e83943a8\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n"
     ]
    }
   ],
   "source": [
    "# !pip install sqlalchemy\n",
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.4\n"
     ]
    }
   ],
   "source": [
    "# Check Pandas \n",
    "\n",
    "print(pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-24 10:36:14--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/ea580e9e-555c-4bd0-ae73-43051d8e7c0b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250124T103614Z&X-Amz-Expires=300&X-Amz-Signature=76ed1d1c8f4f078cc8e05569204f222ec64cdf5067cdba8c1665123d7fbdec6a&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-01-24 10:36:14--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/ea580e9e-555c-4bd0-ae73-43051d8e7c0b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250124T103614Z&X-Amz-Expires=300&X-Amz-Signature=76ed1d1c8f4f078cc8e05569204f222ec64cdf5067cdba8c1665123d7fbdec6a&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8262584 (7.9M) [application/octet-stream]\n",
      "Saving to: ‘green_tripdata_2019-10.csv.gz’\n",
      "\n",
      "green_tripdata_2019 100%[===================>]   7.88M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-01-24 10:36:14 (277 MB/s) - ‘green_tripdata_2019-10.csv.gz’ saved [8262584/8262584]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-24 10:36:31--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250124T103631Z&X-Amz-Expires=300&X-Amz-Signature=ef30c4573db2bee7e384d9014e9400a962cb726f88d99bf9c697c98784464fc6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-01-24 10:36:31--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250124T103631Z&X-Amz-Expires=300&X-Amz-Signature=ef30c4573db2bee7e384d9014e9400a962cb726f88d99bf9c697c98784464fc6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-24 10:36:31 (115 MB/s) - ‘taxi_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23357/4077832265.py:4: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-10-01 00:26:02</td>\n",
       "      <td>2019-10-01 00:39:58</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112</td>\n",
       "      <td>196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-10-01 00:18:11</td>\n",
       "      <td>2019-10-01 00:22:38</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43</td>\n",
       "      <td>263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-10-01 00:09:31</td>\n",
       "      <td>2019-10-01 00:24:47</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255</td>\n",
       "      <td>228</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>22.80</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-10-01 00:37:40</td>\n",
       "      <td>2019-10-01 00:41:49</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.80</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-10-01 00:08:13</td>\n",
       "      <td>2019-10-01 00:17:56</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97</td>\n",
       "      <td>188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.52</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "0       2.0  2019-10-01 00:26:02   2019-10-01 00:39:58                  N   \n",
       "1       1.0  2019-10-01 00:18:11   2019-10-01 00:22:38                  N   \n",
       "2       1.0  2019-10-01 00:09:31   2019-10-01 00:24:47                  N   \n",
       "3       1.0  2019-10-01 00:37:40   2019-10-01 00:41:49                  N   \n",
       "4       2.0  2019-10-01 00:08:13   2019-10-01 00:17:56                  N   \n",
       "\n",
       "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "0         1.0           112           196              1.0           5.88   \n",
       "1         1.0            43           263              1.0           0.80   \n",
       "2         1.0           255           228              2.0           7.50   \n",
       "3         1.0           181           181              1.0           0.90   \n",
       "4         1.0            97           188              1.0           2.52   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  ehail_fee  \\\n",
       "0         18.0   0.50      0.5        0.00           0.0        NaN   \n",
       "1          5.0   3.25      0.5        0.00           0.0        NaN   \n",
       "2         21.5   0.50      0.5        0.00           0.0        NaN   \n",
       "3          5.5   0.50      0.5        0.00           0.0        NaN   \n",
       "4         10.0   0.50      0.5        2.26           0.0        NaN   \n",
       "\n",
       "   improvement_surcharge  total_amount  payment_type  trip_type  \\\n",
       "0                    0.3         19.30           2.0        1.0   \n",
       "1                    0.3          9.05           2.0        1.0   \n",
       "2                    0.3         22.80           2.0        1.0   \n",
       "3                    0.3          6.80           2.0        1.0   \n",
       "4                    0.3         13.56           1.0        1.0   \n",
       "\n",
       "   congestion_surcharge  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " file_path = \"/teamspace/studios/this_studio/module-1/lesson-2/green_tripdata_2019-10.csv.gz\"\n",
    "\n",
    " with gzip.open(file_path, \"rb\") as file:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"green_taxi_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"green_taxi_data\" (\n",
      "\"VendorID\" REAL,\n",
      "  \"lpep_pickup_datetime\" TEXT,\n",
      "  \"lpep_dropoff_datetime\" TEXT,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"RatecodeID\" REAL,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"passenger_count\" REAL,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"ehail_fee\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"payment_type\" REAL,\n",
      "  \"trip_type\" REAL,\n",
      "  \"congestion_surcharge\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name=\"green_taxi_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"green_taxi_data\" (\n",
      "\"VendorID\" REAL,\n",
      "  \"lpep_pickup_datetime\" TIMESTAMP,\n",
      "  \"lpep_dropoff_datetime\" TIMESTAMP,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"RatecodeID\" REAL,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"passenger_count\" REAL,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"ehail_fee\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"payment_type\" REAL,\n",
      "  \"trip_type\" REAL,\n",
      "  \"congestion_surcharge\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name=\"green_taxi_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/lib/x86_64-linux-gnu/libp11-kit.so.0: undefined symbol: ffi_type_pointer, version LIBFFI_BASE_7.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m connection_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgresql://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a SQLAlchemy engine\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a pandas DataFrame\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# csv_file_path = \"data.csv\"  # Replace with your CSV file path\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv(csv_file_path)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(pd.io.sql.get_schema(df, name=\"green_taxi_data\", con=engine))\u001b[39;00m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sqlalchemy/util/deprecations.py:281\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    275\u001b[0m         _warn_with_version(\n\u001b[1;32m    276\u001b[0m             messages[m],\n\u001b[1;32m    277\u001b[0m             versions[m],\n\u001b[1;32m    278\u001b[0m             version_warnings[m],\n\u001b[1;32m    279\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    280\u001b[0m         )\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sqlalchemy/engine/create.py:602\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    601\u001b[0m             dbapi_args[k] \u001b[38;5;241m=\u001b[39m pop_kwarg(k)\n\u001b[0;32m--> 602\u001b[0m     dbapi \u001b[38;5;241m=\u001b[39m \u001b[43mdbapi_meth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdbapi_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m dialect_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbapi\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dbapi\n\u001b[1;32m    606\u001b[0m dialect_args\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_linting\u001b[39m\u001b[38;5;124m\"\u001b[39m, compiler\u001b[38;5;241m.\u001b[39mNO_LINTING)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py:696\u001b[0m, in \u001b[0;36mPGDialect_psycopg2.import_dbapi\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimport_dbapi\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m--> 696\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m psycopg2\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/psycopg2/__init__.py:51\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"A Python driver for PostgreSQL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mpsycopg is a PostgreSQL_ database adapter for the Python_ programming\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    TimeFromTicks, Timestamp, TimestampFromTicks\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# psycopg/__init__.py - initialization of the psycopg module\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2003-2019 Federico Di Gregorio  <fog@debian.org>\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Import the DBAPI-2.0 stuff into top-level module.\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpsycopg2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_psycopg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (                     \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     BINARY, NUMBER, STRING, DATETIME, ROWID,\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m     Binary, Date, Time, Timestamp,\n\u001b[1;32m     55\u001b[0m     DateFromTicks, TimeFromTicks, TimestampFromTicks,\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m     Error, \u001b[38;5;167;01mWarning\u001b[39;00m, DataError, DatabaseError, ProgrammingError, IntegrityError,\n\u001b[1;32m     58\u001b[0m     InterfaceError, InternalError, NotSupportedError, OperationalError,\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m     _connect, apilevel, threadsafety, paramstyle,\n\u001b[1;32m     61\u001b[0m     __version__, __libpq_version__,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Register default adapters.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpsycopg2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extensions \u001b[38;5;28;01mas\u001b[39;00m _ext\n",
      "\u001b[0;31mImportError\u001b[0m: /lib/x86_64-linux-gnu/libp11-kit.so.0: undefined symbol: ffi_type_pointer, version LIBFFI_BASE_7.0"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define PostgreSQL connection details\n",
    "db_config = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"host\": \"localhost\",  # Use the host of your PostgreSQL database\n",
    "    \"port\": \"5433\",       # Adjust the port if different\n",
    "    \"database\": \"ny_taxi\"\n",
    "}\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "# csv_file_path = \"data.csv\"  # Replace with your CSV file path\n",
    "# df = pd.read_csv(csv_file_path)\n",
    "# print(pd.io.sql.get_schema(df, name=\"green_taxi_data\", con=engine))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Send the DataFrame to the PostgreSQL database\u001b[39;00m\n\u001b[1;32m      2\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_green_taxi\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your target table name\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df\u001b[38;5;241m.\u001b[39mto_sql(table_name, \u001b[43mengine\u001b[49m, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m successfully inserted into the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m table.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'engine' is not defined"
     ]
    }
   ],
   "source": [
    "# Send the DataFrame to the PostgreSQL database\n",
    "table_name = \"table_green_taxi\"  # Replace with your target table name\n",
    "df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"Data from {csv_file_path} successfully inserted into the {table_name} table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table DIM_taxi_zone created and data from /teamspace/studios/this_studio/module-1/lesson-2/taxi_zone_lookup.csv successfully inserted.\n"
     ]
    }
   ],
   "source": [
    "# Define PostgreSQL connection details\n",
    "db_config = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5433\",\n",
    "    \"database\": \"ny_taxi\"\n",
    "}\n",
    "\n",
    "# Establish connection\n",
    "conn = psycopg.connect(\n",
    "    user=db_config[\"user\"],\n",
    "    password=db_config[\"password\"],\n",
    "    host=db_config[\"host\"],\n",
    "    port=db_config[\"port\"],\n",
    "    dbname=db_config[\"database\"]\n",
    ")\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "csv_file_path = \"/teamspace/studios/this_studio/module-1/lesson-2/taxi_zone_lookup.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Define the table name\n",
    "table_name = \"DIM_taxi_zone\"\n",
    "\n",
    "# Create the table schema based on the DataFrame columns\n",
    "# Map pandas dtypes to PostgreSQL data types\n",
    "dtype_mapping = {\n",
    "    \"int64\": \"BIGINT\",\n",
    "    \"float64\": \"DOUBLE PRECISION\",\n",
    "    \"object\": \"TEXT\",\n",
    "    \"datetime64[ns]\": \"TIMESTAMP\",\n",
    "    \"bool\": \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "# Generate the CREATE TABLE SQL statement\n",
    "columns_with_types = []\n",
    "for column, dtype in df.dtypes.items():\n",
    "    postgres_type = dtype_mapping.get(str(dtype), \"TEXT\")  # Default to TEXT if dtype is not mapped\n",
    "    columns_with_types.append(f\"{column} {postgres_type}\")\n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    {\", \".join(columns_with_types)}\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the CREATE TABLE query\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Prepare and execute the INSERT query\n",
    "columns = \", \".join(df.columns)\n",
    "placeholders = \", \".join([\"%s\"] * len(df.columns))\n",
    "\n",
    "insert_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "# Insert each row from the DataFrame\n",
    "for row in df.itertuples(index=False, name=None):\n",
    "    cursor.execute(insert_query, row)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Table {table_name} created and data from {csv_file_path} successfully inserted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Distance Range  Number of Trips\n",
      "0            up_to_1_mile           104830\n",
      "1   between_1_and_3_miles           198995\n",
      "2   between_3_and_7_miles           109642\n",
      "3  between_7_and_10_miles            27686\n",
      "4           over_10_miles            35201\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for the specified date range\n",
    "start_date = \"2019-10-01\"\n",
    "end_date = \"2019-11-01\"\n",
    "filtered_df = df[(df[\"lpep_pickup_datetime\"] >= start_date) & (df[\"lpep_pickup_datetime\"] < end_date)]\n",
    "\n",
    "# Define the distance ranges and count trips in each range\n",
    "distance_ranges = {\n",
    "    \"up_to_1_mile\": filtered_df[filtered_df[\"trip_distance\"] <= 1].shape[0],\n",
    "    \"between_1_and_3_miles\": filtered_df[(filtered_df[\"trip_distance\"] > 1) & (filtered_df[\"trip_distance\"] <= 3)].shape[0],\n",
    "    \"between_3_and_7_miles\": filtered_df[(filtered_df[\"trip_distance\"] > 3) & (filtered_df[\"trip_distance\"] <= 7)].shape[0],\n",
    "    \"between_7_and_10_miles\": filtered_df[(filtered_df[\"trip_distance\"] > 7) & (filtered_df[\"trip_distance\"] <= 10)].shape[0],\n",
    "    \"over_10_miles\": filtered_df[filtered_df[\"trip_distance\"] > 10].shape[0]\n",
    "}\n",
    "\n",
    "# Convert the results to a DataFrame for better visualization\n",
    "result_df = pd.DataFrame(list(distance_ranges.items()), columns=[\"Distance Range\", \"Number of Trips\"])\n",
    "\n",
    "# Display the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LocationID', 'Borough', 'Zone', 'service_zone'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
